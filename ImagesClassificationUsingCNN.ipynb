{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7aa92a",
   "metadata": {
    "id": "ed7aa92a"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1a1f",
   "metadata": {
    "id": "e61a1a1f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08625ad9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08625ad9",
    "outputId": "ccbaf6f6-fcd0-46ed-fd9a-c3135ab4f63d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0c2d5",
   "metadata": {
    "id": "6cd0c2d5"
   },
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e6cf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "723e6cf6",
    "outputId": "4d15ac22-0fe0-4c1a-d32d-94ddfb2335b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # to introduce variation in image size and position\n",
    "    transforms.RandomHorizontalFlip(),     # to account for variability in image orientations\n",
    "    transforms.ToTensor(),                 \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                 \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  \n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0eb6b9",
   "metadata": {
    "id": "3e0eb6b9"
   },
   "source": [
    "# CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36f96a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a36f96a",
    "outputId": "5a4dfef7-16f1-4096-b10e-83d57abd983b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "\n",
    "        # convolutional layer\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #1st set of conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        #2nd set of conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        #flatten before fully connected layers\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "\n",
    "        #fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "cnn =CNN().to(device)\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63628cd9",
   "metadata": {
    "id": "63628cd9"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adca22",
   "metadata": {
    "id": "a0adca22"
   },
   "source": [
    "# Training CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644c3df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d644c3df",
    "outputId": "e73dd2a0-20eb-4818-ae28-6c78f8abc5f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.498\n",
      "[1,  4000] loss: 1.468\n",
      "[1,  6000] loss: 1.433\n",
      "[1,  8000] loss: 1.410\n",
      "[1, 10000] loss: 1.380\n",
      "[1, 12000] loss: 1.381\n",
      "[2,  2000] loss: 1.300\n",
      "[2,  4000] loss: 1.304\n",
      "[2,  6000] loss: 1.272\n",
      "[2,  8000] loss: 1.248\n",
      "[2, 10000] loss: 1.256\n",
      "[2, 12000] loss: 1.252\n",
      "[3,  2000] loss: 1.189\n",
      "[3,  4000] loss: 1.208\n",
      "[3,  6000] loss: 1.176\n",
      "[3,  8000] loss: 1.173\n",
      "[3, 10000] loss: 1.168\n",
      "[3, 12000] loss: 1.152\n",
      "[4,  2000] loss: 1.079\n",
      "[4,  4000] loss: 1.108\n",
      "[4,  6000] loss: 1.109\n",
      "[4,  8000] loss: 1.112\n",
      "[4, 10000] loss: 1.120\n",
      "[4, 12000] loss: 1.094\n",
      "[5,  2000] loss: 1.026\n",
      "[5,  4000] loss: 1.049\n",
      "[5,  6000] loss: 1.021\n",
      "[5,  8000] loss: 1.064\n",
      "[5, 10000] loss: 1.043\n",
      "[5, 12000] loss: 1.049\n",
      "[6,  2000] loss: 0.940\n",
      "[6,  4000] loss: 0.996\n",
      "[6,  6000] loss: 0.985\n",
      "[6,  8000] loss: 1.005\n",
      "[6, 10000] loss: 1.014\n",
      "[6, 12000] loss: 0.994\n",
      "[7,  2000] loss: 0.933\n",
      "[7,  4000] loss: 0.940\n",
      "[7,  6000] loss: 0.921\n",
      "[7,  8000] loss: 0.947\n",
      "[7, 10000] loss: 0.959\n",
      "[7, 12000] loss: 0.988\n",
      "[8,  2000] loss: 0.870\n",
      "[8,  4000] loss: 0.901\n",
      "[8,  6000] loss: 0.923\n",
      "[8,  8000] loss: 0.932\n",
      "[8, 10000] loss: 0.951\n",
      "[8, 12000] loss: 0.921\n",
      "[9,  2000] loss: 0.831\n",
      "[9,  4000] loss: 0.880\n",
      "[9,  6000] loss: 0.876\n",
      "[9,  8000] loss: 0.868\n",
      "[9, 10000] loss: 0.899\n",
      "[9, 12000] loss: 0.921\n",
      "[10,  2000] loss: 0.793\n",
      "[10,  4000] loss: 0.842\n",
      "[10,  6000] loss: 0.857\n",
      "[10,  8000] loss: 0.850\n",
      "[10, 10000] loss: 0.879\n",
      "[10, 12000] loss: 0.889\n",
      "[11,  2000] loss: 0.775\n",
      "[11,  4000] loss: 0.793\n",
      "[11,  6000] loss: 0.839\n",
      "[11,  8000] loss: 0.850\n",
      "[11, 10000] loss: 0.863\n",
      "[11, 12000] loss: 0.848\n",
      "[12,  2000] loss: 0.749\n",
      "[12,  4000] loss: 0.780\n",
      "[12,  6000] loss: 0.807\n",
      "[12,  8000] loss: 0.809\n",
      "[12, 10000] loss: 0.823\n",
      "[12, 12000] loss: 0.813\n",
      "[13,  2000] loss: 0.722\n",
      "[13,  4000] loss: 0.772\n",
      "[13,  6000] loss: 0.785\n",
      "[13,  8000] loss: 0.788\n",
      "[13, 10000] loss: 0.824\n",
      "[13, 12000] loss: 0.808\n",
      "[14,  2000] loss: 0.679\n",
      "[14,  4000] loss: 0.718\n",
      "[14,  6000] loss: 0.753\n",
      "[14,  8000] loss: 0.786\n",
      "[14, 10000] loss: 0.788\n",
      "[14, 12000] loss: 0.813\n",
      "[15,  2000] loss: 0.662\n",
      "[15,  4000] loss: 0.711\n",
      "[15,  6000] loss: 0.756\n",
      "[15,  8000] loss: 0.759\n",
      "[15, 10000] loss: 0.790\n",
      "[15, 12000] loss: 0.778\n",
      "[16,  2000] loss: 0.676\n",
      "[16,  4000] loss: 0.689\n",
      "[16,  6000] loss: 0.722\n",
      "[16,  8000] loss: 0.733\n",
      "[16, 10000] loss: 0.779\n",
      "[16, 12000] loss: 0.771\n",
      "[17,  2000] loss: 0.663\n",
      "[17,  4000] loss: 0.676\n",
      "[17,  6000] loss: 0.704\n",
      "[17,  8000] loss: 0.740\n",
      "[17, 10000] loss: 0.743\n",
      "[17, 12000] loss: 0.751\n",
      "[18,  2000] loss: 0.667\n",
      "[18,  4000] loss: 0.673\n",
      "[18,  6000] loss: 0.694\n",
      "[18,  8000] loss: 0.726\n",
      "[18, 10000] loss: 0.734\n",
      "[18, 12000] loss: 0.740\n",
      "[19,  2000] loss: 0.632\n",
      "[19,  4000] loss: 0.643\n",
      "[19,  6000] loss: 0.693\n",
      "[19,  8000] loss: 0.703\n",
      "[19, 10000] loss: 0.727\n",
      "[19, 12000] loss: 0.732\n",
      "[20,  2000] loss: 0.622\n",
      "[20,  4000] loss: 0.667\n",
      "[20,  6000] loss: 0.678\n",
      "[20,  8000] loss: 0.700\n",
      "[20, 10000] loss: 0.709\n",
      "[20, 12000] loss: 0.736\n",
      "[21,  2000] loss: 0.616\n",
      "[21,  4000] loss: 0.646\n",
      "[21,  6000] loss: 0.676\n",
      "[21,  8000] loss: 0.694\n",
      "[21, 10000] loss: 0.704\n",
      "[21, 12000] loss: 0.728\n",
      "[22,  2000] loss: 0.602\n",
      "[22,  4000] loss: 0.617\n",
      "[22,  6000] loss: 0.681\n",
      "[22,  8000] loss: 0.669\n",
      "[22, 10000] loss: 0.725\n",
      "[22, 12000] loss: 0.688\n",
      "[23,  2000] loss: 0.584\n",
      "[23,  4000] loss: 0.601\n",
      "[23,  6000] loss: 0.658\n",
      "[23,  8000] loss: 0.682\n",
      "[23, 10000] loss: 0.689\n",
      "[23, 12000] loss: 0.705\n",
      "[24,  2000] loss: 0.603\n",
      "[24,  4000] loss: 0.622\n",
      "[24,  6000] loss: 0.669\n",
      "[24,  8000] loss: 0.665\n",
      "[24, 10000] loss: 0.679\n",
      "[24, 12000] loss: 0.700\n",
      "[25,  2000] loss: 0.569\n",
      "[25,  4000] loss: 0.615\n",
      "[25,  6000] loss: 0.637\n",
      "[25,  8000] loss: 0.670\n",
      "[25, 10000] loss: 0.685\n",
      "[25, 12000] loss: 0.703\n",
      "[26,  2000] loss: 0.563\n",
      "[26,  4000] loss: 0.604\n",
      "[26,  6000] loss: 0.634\n",
      "[26,  8000] loss: 0.685\n",
      "[26, 10000] loss: 0.683\n",
      "[26, 12000] loss: 0.707\n",
      "[27,  2000] loss: 0.553\n",
      "[27,  4000] loss: 0.569\n",
      "[27,  6000] loss: 0.639\n",
      "[27,  8000] loss: 0.654\n",
      "[27, 10000] loss: 0.697\n",
      "[27, 12000] loss: 0.685\n",
      "[28,  2000] loss: 0.574\n",
      "[28,  4000] loss: 0.609\n",
      "[28,  6000] loss: 0.651\n",
      "[28,  8000] loss: 0.665\n",
      "[28, 10000] loss: 0.649\n",
      "[28, 12000] loss: 0.668\n",
      "[29,  2000] loss: 0.547\n",
      "[29,  4000] loss: 0.610\n",
      "[29,  6000] loss: 0.633\n",
      "[29,  8000] loss: 0.655\n",
      "[29, 10000] loss: 0.666\n",
      "[29, 12000] loss: 0.670\n",
      "[30,  2000] loss: 0.562\n",
      "[30,  4000] loss: 0.598\n",
      "[30,  6000] loss: 0.644\n",
      "[30,  8000] loss: 0.636\n",
      "[30, 10000] loss: 0.677\n",
      "[30, 12000] loss: 0.679\n",
      "[31,  2000] loss: 0.531\n",
      "[31,  4000] loss: 0.615\n",
      "[31,  6000] loss: 0.610\n",
      "[31,  8000] loss: 0.634\n",
      "[31, 10000] loss: 0.693\n",
      "[31, 12000] loss: 0.676\n",
      "[32,  2000] loss: 0.544\n",
      "[32,  4000] loss: 0.578\n",
      "[32,  6000] loss: 0.647\n",
      "[32,  8000] loss: 0.618\n",
      "[32, 10000] loss: 0.662\n",
      "[32, 12000] loss: 0.664\n",
      "[33,  2000] loss: 0.548\n",
      "[33,  4000] loss: 0.583\n",
      "[33,  6000] loss: 0.590\n",
      "[33,  8000] loss: 0.636\n",
      "[33, 10000] loss: 0.656\n",
      "[33, 12000] loss: 0.676\n",
      "[34,  2000] loss: 0.529\n",
      "[34,  4000] loss: 0.588\n",
      "[34,  6000] loss: 0.619\n",
      "[34,  8000] loss: 0.626\n",
      "[34, 10000] loss: 0.659\n",
      "[34, 12000] loss: 0.639\n",
      "[35,  2000] loss: 0.533\n",
      "[35,  4000] loss: 0.583\n",
      "[35,  6000] loss: 0.603\n",
      "[35,  8000] loss: 0.648\n",
      "[35, 10000] loss: 0.662\n",
      "[35, 12000] loss: 0.688\n",
      "[36,  2000] loss: 0.543\n",
      "[36,  4000] loss: 0.609\n",
      "[36,  6000] loss: 0.619\n",
      "[36,  8000] loss: 0.603\n",
      "[36, 10000] loss: 0.652\n",
      "[36, 12000] loss: 0.643\n",
      "[37,  2000] loss: 0.551\n",
      "[37,  4000] loss: 0.603\n",
      "[37,  6000] loss: 0.624\n",
      "[37,  8000] loss: 0.633\n",
      "[37, 10000] loss: 0.626\n",
      "[37, 12000] loss: 0.660\n",
      "[38,  2000] loss: 0.560\n",
      "[38,  4000] loss: 0.618\n",
      "[38,  6000] loss: 0.622\n",
      "[38,  8000] loss: 0.614\n",
      "[38, 10000] loss: 0.646\n",
      "[38, 12000] loss: 0.690\n",
      "[39,  2000] loss: 0.547\n",
      "[39,  4000] loss: 0.551\n",
      "[39,  6000] loss: 0.608\n",
      "[39,  8000] loss: 0.616\n",
      "[39, 10000] loss: 0.669\n",
      "[39, 12000] loss: 0.671\n",
      "[40,  2000] loss: 0.541\n",
      "[40,  4000] loss: 0.556\n",
      "[40,  6000] loss: 0.621\n",
      "[40,  8000] loss: 0.645\n",
      "[40, 10000] loss: 0.641\n",
      "[40, 12000] loss: 0.665\n",
      "[41,  2000] loss: 0.550\n",
      "[41,  4000] loss: 0.564\n",
      "[41,  6000] loss: 0.582\n",
      "[41,  8000] loss: 0.630\n",
      "[41, 10000] loss: 0.654\n",
      "[41, 12000] loss: 0.704\n",
      "[42,  2000] loss: 0.523\n",
      "[42,  4000] loss: 0.605\n",
      "[42,  6000] loss: 0.634\n",
      "[42,  8000] loss: 0.624\n",
      "[42, 10000] loss: 0.640\n",
      "[42, 12000] loss: 0.669\n",
      "[43,  2000] loss: 0.544\n",
      "[43,  4000] loss: 0.595\n",
      "[43,  6000] loss: 0.593\n",
      "[43,  8000] loss: 0.578\n",
      "[43, 10000] loss: 0.673\n",
      "[43, 12000] loss: 0.655\n",
      "[44,  2000] loss: 0.542\n",
      "[44,  4000] loss: 0.574\n",
      "[44,  6000] loss: 0.600\n",
      "[44,  8000] loss: 0.642\n",
      "[44, 10000] loss: 0.614\n",
      "[44, 12000] loss: 0.651\n",
      "[45,  2000] loss: 0.530\n",
      "[45,  4000] loss: 0.585\n",
      "[45,  6000] loss: 0.621\n",
      "[45,  8000] loss: 0.617\n",
      "[45, 10000] loss: 0.624\n",
      "[45, 12000] loss: 0.651\n",
      "[46,  2000] loss: 0.548\n",
      "[46,  4000] loss: 0.623\n",
      "[46,  6000] loss: 0.623\n",
      "[46,  8000] loss: 0.612\n",
      "[46, 10000] loss: 0.619\n",
      "[46, 12000] loss: 0.684\n",
      "[47,  2000] loss: 0.559\n",
      "[47,  4000] loss: 0.589\n",
      "[47,  6000] loss: 0.598\n",
      "[47,  8000] loss: 0.634\n",
      "[47, 10000] loss: 0.673\n",
      "[47, 12000] loss: 0.690\n",
      "[48,  2000] loss: 0.529\n",
      "[48,  4000] loss: 0.578\n",
      "[48,  6000] loss: 0.593\n",
      "[48,  8000] loss: 0.605\n",
      "[48, 10000] loss: 0.617\n",
      "[48, 12000] loss: 0.654\n",
      "[49,  2000] loss: 0.552\n",
      "[49,  4000] loss: 0.589\n",
      "[49,  6000] loss: 0.619\n",
      "[49,  8000] loss: 0.605\n",
      "[49, 10000] loss: 0.639\n",
      "[49, 12000] loss: 0.651\n",
      "[50,  2000] loss: 0.560\n",
      "[50,  4000] loss: 0.580\n",
      "[50,  6000] loss: 0.593\n",
      "[50,  8000] loss: 0.626\n",
      "[50, 10000] loss: 0.630\n",
      "[50, 12000] loss: 0.658\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  \n",
    "\n",
    "    runningLoss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn(inputs).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runningLoss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, runningLoss / 2000))\n",
    "            runningLoss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba591a",
   "metadata": {
    "id": "2bba591a"
   },
   "source": [
    "# Evaluating CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb5729a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adb5729a",
    "outputId": "5e289fa0-7299-4ac8-e89c-25176c792612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.025660\n",
      "\n",
      "Test Accuracy of  plane: 76% (760/1000)\n",
      "Test Accuracy of    car: 90% (900/1000)\n",
      "Test Accuracy of   bird: 63% (630/1000)\n",
      "Test Accuracy of    cat: 56% (560/1000)\n",
      "Test Accuracy of   deer: 71% (710/1000)\n",
      "Test Accuracy of    dog: 71% (710/1000)\n",
      "Test Accuracy of   frog: 85% (850/1000)\n",
      "Test Accuracy of  horse: 74% (740/1000)\n",
      "Test Accuracy of   ship: 89% (890/1000)\n",
      "Test Accuracy of  truck: 81% (810/1000)\n",
      "\n",
      "Test Accuracy (Overall): 75.6% (7560/10000)\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()\n",
    "\n",
    "testLoss = 0.0\n",
    "classCorrect = list(0. for i in range(10))\n",
    "classTotal = list(0. for i in range(10))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        testLoss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        correctTensor = predicted.eq(labels.data.view_as(predicted))\n",
    "        correct = np.squeeze(correctTensor.cpu().numpy())\n",
    "\n",
    "        for i in range(4):  \n",
    "            label = labels.data[i]\n",
    "            classCorrect[label] += correct[i].item()\n",
    "            classTotal[label] += 1\n",
    "\n",
    "testLoss /= len(testloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(testLoss))\n",
    "\n",
    "for i in range(10):\n",
    "    if classTotal[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * classCorrect[i] / classTotal[i],\n",
    "            np.sum(classCorrect[i]), np.sum(classTotal[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(classCorrect) / np.sum(classTotal),\n",
    "    np.sum(classCorrect), np.sum(classTotal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b5d93",
   "metadata": {},
   "source": [
    "# Now Using Pretrained Model vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283a57f0",
   "metadata": {
    "id": "283a57f0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "EvfIiWLe0s86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvfIiWLe0s86",
    "outputId": "0b96064b-b087-42e3-9513-87a2e35a2170"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683cd978",
   "metadata": {},
   "source": [
    "#  Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "VTeYXDu20s_a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTeYXDu20s_a",
    "outputId": "48a6889e-6cb3-40b9-de78-6e9be27ac99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:15<00:00, 11107494.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize for VGG16 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainSet = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "testSet = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(testSet, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed6d84",
   "metadata": {},
   "source": [
    "# Load VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uODfp9iT0tBs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uODfp9iT0tBs",
    "outputId": "93e79add-8b08-45d9-bece-d09f2c9f52e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:04<00:00, 125MB/s]\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# modifying final layer to match the number of classes in CIFAR 10\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 10)\n",
    "vgg16 = vgg16.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845215e8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4NFQwaFp0tEN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NFQwaFp0tEN",
    "outputId": "0cd233ae-88cf-45d4-c5da-6e8eda8701ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Mini-batch 100] loss: 1.188\n",
      "[Epoch 1, Mini-batch 200] loss: 0.630\n",
      "[Epoch 1, Mini-batch 300] loss: 0.565\n",
      "[Epoch 1, Mini-batch 400] loss: 0.460\n",
      "[Epoch 1, Mini-batch 500] loss: 0.424\n",
      "[Epoch 1, Mini-batch 600] loss: 0.439\n",
      "[Epoch 1, Mini-batch 700] loss: 0.427\n",
      "[Epoch 1, Mini-batch 800] loss: 0.382\n",
      "[Epoch 1, Mini-batch 900] loss: 0.374\n",
      "[Epoch 1, Mini-batch 1000] loss: 0.357\n",
      "[Epoch 1, Mini-batch 1100] loss: 0.359\n",
      "[Epoch 1, Mini-batch 1200] loss: 0.326\n",
      "[Epoch 1, Mini-batch 1300] loss: 0.319\n",
      "[Epoch 1, Mini-batch 1400] loss: 0.313\n",
      "[Epoch 1, Mini-batch 1500] loss: 0.291\n",
      "[Epoch 2, Mini-batch 100] loss: 0.246\n",
      "[Epoch 2, Mini-batch 200] loss: 0.254\n",
      "[Epoch 2, Mini-batch 300] loss: 0.215\n",
      "[Epoch 2, Mini-batch 400] loss: 0.212\n",
      "[Epoch 2, Mini-batch 500] loss: 0.234\n",
      "[Epoch 2, Mini-batch 600] loss: 0.222\n",
      "[Epoch 2, Mini-batch 700] loss: 0.209\n",
      "[Epoch 2, Mini-batch 800] loss: 0.205\n",
      "[Epoch 2, Mini-batch 900] loss: 0.209\n",
      "[Epoch 2, Mini-batch 1000] loss: 0.203\n",
      "[Epoch 2, Mini-batch 1100] loss: 0.230\n",
      "[Epoch 2, Mini-batch 1200] loss: 0.218\n",
      "[Epoch 2, Mini-batch 1300] loss: 0.217\n",
      "[Epoch 2, Mini-batch 1400] loss: 0.216\n",
      "[Epoch 2, Mini-batch 1500] loss: 0.204\n",
      "[Epoch 3, Mini-batch 100] loss: 0.140\n",
      "[Epoch 3, Mini-batch 200] loss: 0.133\n",
      "[Epoch 3, Mini-batch 300] loss: 0.127\n",
      "[Epoch 3, Mini-batch 400] loss: 0.115\n",
      "[Epoch 3, Mini-batch 500] loss: 0.145\n",
      "[Epoch 3, Mini-batch 600] loss: 0.151\n",
      "[Epoch 3, Mini-batch 700] loss: 0.151\n",
      "[Epoch 3, Mini-batch 800] loss: 0.138\n",
      "[Epoch 3, Mini-batch 900] loss: 0.151\n",
      "[Epoch 3, Mini-batch 1000] loss: 0.146\n",
      "[Epoch 3, Mini-batch 1100] loss: 0.141\n",
      "[Epoch 3, Mini-batch 1200] loss: 0.128\n",
      "[Epoch 3, Mini-batch 1300] loss: 0.164\n",
      "[Epoch 3, Mini-batch 1400] loss: 0.157\n",
      "[Epoch 3, Mini-batch 1500] loss: 0.144\n",
      "[Epoch 4, Mini-batch 100] loss: 0.095\n",
      "[Epoch 4, Mini-batch 200] loss: 0.093\n",
      "[Epoch 4, Mini-batch 300] loss: 0.091\n",
      "[Epoch 4, Mini-batch 400] loss: 0.081\n",
      "[Epoch 4, Mini-batch 500] loss: 0.105\n",
      "[Epoch 4, Mini-batch 600] loss: 0.099\n",
      "[Epoch 4, Mini-batch 700] loss: 0.089\n",
      "[Epoch 4, Mini-batch 800] loss: 0.095\n",
      "[Epoch 4, Mini-batch 900] loss: 0.086\n",
      "[Epoch 4, Mini-batch 1000] loss: 0.078\n",
      "[Epoch 4, Mini-batch 1100] loss: 0.097\n",
      "[Epoch 4, Mini-batch 1200] loss: 0.101\n",
      "[Epoch 4, Mini-batch 1300] loss: 0.105\n",
      "[Epoch 4, Mini-batch 1400] loss: 0.095\n",
      "[Epoch 4, Mini-batch 1500] loss: 0.110\n",
      "[Epoch 5, Mini-batch 100] loss: 0.044\n",
      "[Epoch 5, Mini-batch 200] loss: 0.040\n",
      "[Epoch 5, Mini-batch 300] loss: 0.045\n",
      "[Epoch 5, Mini-batch 400] loss: 0.061\n",
      "[Epoch 5, Mini-batch 500] loss: 0.070\n",
      "[Epoch 5, Mini-batch 600] loss: 0.069\n",
      "[Epoch 5, Mini-batch 700] loss: 0.079\n",
      "[Epoch 5, Mini-batch 800] loss: 0.070\n",
      "[Epoch 5, Mini-batch 900] loss: 0.063\n",
      "[Epoch 5, Mini-batch 1000] loss: 0.076\n",
      "[Epoch 5, Mini-batch 1100] loss: 0.058\n",
      "[Epoch 5, Mini-batch 1200] loss: 0.049\n",
      "[Epoch 5, Mini-batch 1300] loss: 0.066\n",
      "[Epoch 5, Mini-batch 1400] loss: 0.068\n",
      "[Epoch 5, Mini-batch 1500] loss: 0.084\n",
      "[Epoch 6, Mini-batch 100] loss: 0.051\n",
      "[Epoch 6, Mini-batch 200] loss: 0.028\n",
      "[Epoch 6, Mini-batch 300] loss: 0.041\n",
      "[Epoch 6, Mini-batch 400] loss: 0.049\n",
      "[Epoch 6, Mini-batch 500] loss: 0.040\n",
      "[Epoch 6, Mini-batch 600] loss: 0.041\n",
      "[Epoch 6, Mini-batch 700] loss: 0.043\n",
      "[Epoch 6, Mini-batch 800] loss: 0.054\n",
      "[Epoch 6, Mini-batch 900] loss: 0.046\n",
      "[Epoch 6, Mini-batch 1000] loss: 0.062\n",
      "[Epoch 6, Mini-batch 1100] loss: 0.037\n",
      "[Epoch 6, Mini-batch 1200] loss: 0.057\n",
      "[Epoch 6, Mini-batch 1300] loss: 0.066\n",
      "[Epoch 6, Mini-batch 1400] loss: 0.044\n",
      "[Epoch 6, Mini-batch 1500] loss: 0.040\n",
      "[Epoch 7, Mini-batch 100] loss: 0.038\n",
      "[Epoch 7, Mini-batch 200] loss: 0.042\n",
      "[Epoch 7, Mini-batch 300] loss: 0.035\n",
      "[Epoch 7, Mini-batch 400] loss: 0.033\n",
      "[Epoch 7, Mini-batch 500] loss: 0.035\n",
      "[Epoch 7, Mini-batch 600] loss: 0.035\n",
      "[Epoch 7, Mini-batch 700] loss: 0.052\n",
      "[Epoch 7, Mini-batch 800] loss: 0.030\n",
      "[Epoch 7, Mini-batch 900] loss: 0.037\n",
      "[Epoch 7, Mini-batch 1000] loss: 0.053\n",
      "[Epoch 7, Mini-batch 1100] loss: 0.033\n",
      "[Epoch 7, Mini-batch 1200] loss: 0.030\n",
      "[Epoch 7, Mini-batch 1300] loss: 0.056\n",
      "[Epoch 7, Mini-batch 1400] loss: 0.027\n",
      "[Epoch 7, Mini-batch 1500] loss: 0.033\n",
      "[Epoch 8, Mini-batch 100] loss: 0.022\n",
      "[Epoch 8, Mini-batch 200] loss: 0.030\n",
      "[Epoch 8, Mini-batch 300] loss: 0.034\n",
      "[Epoch 8, Mini-batch 400] loss: 0.015\n",
      "[Epoch 8, Mini-batch 500] loss: 0.023\n",
      "[Epoch 8, Mini-batch 600] loss: 0.032\n",
      "[Epoch 8, Mini-batch 700] loss: 0.031\n",
      "[Epoch 8, Mini-batch 800] loss: 0.032\n",
      "[Epoch 8, Mini-batch 900] loss: 0.034\n",
      "[Epoch 8, Mini-batch 1000] loss: 0.047\n",
      "[Epoch 8, Mini-batch 1100] loss: 0.041\n",
      "[Epoch 8, Mini-batch 1200] loss: 0.029\n",
      "[Epoch 8, Mini-batch 1300] loss: 0.030\n",
      "[Epoch 8, Mini-batch 1400] loss: 0.042\n",
      "[Epoch 8, Mini-batch 1500] loss: 0.025\n",
      "[Epoch 9, Mini-batch 100] loss: 0.014\n",
      "[Epoch 9, Mini-batch 200] loss: 0.017\n",
      "[Epoch 9, Mini-batch 300] loss: 0.023\n",
      "[Epoch 9, Mini-batch 400] loss: 0.020\n",
      "[Epoch 9, Mini-batch 500] loss: 0.037\n",
      "[Epoch 9, Mini-batch 600] loss: 0.017\n",
      "[Epoch 9, Mini-batch 700] loss: 0.029\n",
      "[Epoch 9, Mini-batch 800] loss: 0.027\n",
      "[Epoch 9, Mini-batch 900] loss: 0.021\n",
      "[Epoch 9, Mini-batch 1000] loss: 0.019\n",
      "[Epoch 9, Mini-batch 1100] loss: 0.023\n",
      "[Epoch 9, Mini-batch 1200] loss: 0.019\n",
      "[Epoch 9, Mini-batch 1300] loss: 0.040\n",
      "[Epoch 9, Mini-batch 1400] loss: 0.029\n",
      "[Epoch 9, Mini-batch 1500] loss: 0.029\n",
      "[Epoch 10, Mini-batch 100] loss: 0.015\n",
      "[Epoch 10, Mini-batch 200] loss: 0.021\n",
      "[Epoch 10, Mini-batch 300] loss: 0.015\n",
      "[Epoch 10, Mini-batch 400] loss: 0.027\n",
      "[Epoch 10, Mini-batch 500] loss: 0.011\n",
      "[Epoch 10, Mini-batch 600] loss: 0.043\n",
      "[Epoch 10, Mini-batch 700] loss: 0.025\n",
      "[Epoch 10, Mini-batch 800] loss: 0.022\n",
      "[Epoch 10, Mini-batch 900] loss: 0.013\n",
      "[Epoch 10, Mini-batch 1000] loss: 0.007\n",
      "[Epoch 10, Mini-batch 1100] loss: 0.012\n",
      "[Epoch 10, Mini-batch 1200] loss: 0.016\n",
      "[Epoch 10, Mini-batch 1300] loss: 0.028\n",
      "[Epoch 10, Mini-batch 1400] loss: 0.015\n",
      "[Epoch 10, Mini-batch 1500] loss: 0.013\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 10\n",
    "for epoch in range(numEpochs):\n",
    "    vgg16.train()\n",
    "    runningLoss = 0.0\n",
    "    for i, data in enumerate(trainLoader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = vgg16(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runningLoss += loss.item()\n",
    "        if i % 100 == 99:  # prints every 100 mini batches\n",
    "            print(f'[Epoch {epoch+1}, Mini-batch {i+1}] loss: {runningLoss / 100:.3f}')\n",
    "            runningLoss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb992e0e",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5221f906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5221f906",
    "outputId": "eab52a71-9ce0-4627-a366-6e37c281236d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.278592\n",
      "\n",
      "Test Accuracy of plane: 95.00% (950/1000)\n",
      "Test Accuracy of car: 97.10% (971/1000)\n",
      "Test Accuracy of bird: 88.80% (888/1000)\n",
      "Test Accuracy of cat: 86.50% (865/1000)\n",
      "Test Accuracy of deer: 92.00% (920/1000)\n",
      "Test Accuracy of dog: 90.10% (901/1000)\n",
      "Test Accuracy of frog: 96.70% (967/1000)\n",
      "Test Accuracy of horse: 96.60% (966/1000)\n",
      "Test Accuracy of ship: 96.50% (965/1000)\n",
      "Test Accuracy of truck: 95.10% (951/1000)\n",
      "\n",
      "Test Accuracy (Overall): 93.44% (9344/10000)\n"
     ]
    }
   ],
   "source": [
    "# class names for CIFAR 10 dataset\n",
    "classNames = [\n",
    "    'plane', 'car', 'bird', 'cat',\n",
    "    'deer', 'dog', 'frog', 'horse',\n",
    "    'ship', 'truck'\n",
    "]\n",
    "\n",
    "vgg16.eval()\n",
    "testLoss = 0.0\n",
    "classCorrect = list(0. for i in range(10))\n",
    "classTotal = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vgg16(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        testLoss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            classCorrect[label] += c[i].item()\n",
    "            classTotal[label] += 1\n",
    "\n",
    "testLoss = testLoss / len(testloader.dataset)\n",
    "print(f'Test Loss: {testLoss:.6f}\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    if classTotal[i] > 0:\n",
    "        print(f'Test Accuracy of {classNames[i]}: {100 * classCorrect[i] / classTotal[i]:.2f}% ({int(classCorrect[i])}/{int(classTotal[i])})')\n",
    "    else:\n",
    "        print(f'Test Accuracy of {classNames[i]}: N/A (no training examples)')\n",
    "\n",
    "print(f'\\nTest Accuracy (Overall): {100. * np.sum(classCorrect) / np.sum(classTotal):.2f}% ({int(np.sum(classCorrect))}/{int(np.sum(classTotal))})')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
